{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "91f0fc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable tensorflow warnings:\n",
    "import os # imports OS library which can perform OS commands\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # sets environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0216f909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Needed to store data as a dataframe for data preprocessing\n",
    "import numpy as np\n",
    "import re # Used to generate regex expressions\n",
    "import tensorflow as tf # Import ML functionality\n",
    "from tensorflow.keras.layers import TextVectorization # Creates a vectoriser -> converts tokens into model embeddings\n",
    "from tensorflow.keras.models import Sequential # Used for creating model based on its layers\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense # Functionality for creating specific layers used in model\n",
    "from sklearn.model_selection import train_test_split # Used for creating a train-test split\n",
    "from sklearn.metrics import classification_report, accuracy_score # Used for performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4366b41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "# Prints version of TensorFlow\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5105e189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/IMDB_Dataset.csv\") # Reads data in the IMDB dataset and converts it into a dataframe\n",
    "print(data.head()) # Prints the first 5 lines of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d39d6237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    one of the other reviewers has mentioned that ...\n",
      "1    a wonderful little production the filming tech...\n",
      "2    i thought this was a wonderful way to spend ti...\n",
      "3    basically there s a family where a little boy ...\n",
      "4    petter mattei s love in the time of money is a...\n",
      "Name: clean_review, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    # Cleans the text\n",
    "    text = text.lower() # Converts text to lower case\n",
    "\n",
    "    # re is a regex library\n",
    "    text = re.sub('<.*?>', '', text) # Removes HTML tags from the text (replaces them with empty string)\n",
    "                                     # <> is a typical structure for HTML tags\n",
    "                                     # . means that any character but new line would be removed\n",
    "                                     # * means 0 or more characters so .* means to remove all characters within <> (i.e.: the full HTML tag)\n",
    "                                     # ? specifies that you want to remove the HTML tags in a non-greedy way -> removes smallest possible pattern (i.e.: does not remove text inbetween HTML tags)\n",
    "\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text) # Removes special characters from the text and replaces them with a space\n",
    "                                          # [^a-zA-Z] Removes any characters that are not (^) lower (a-z) or uppercase (A-Z) letters\n",
    "\n",
    "\n",
    "    text = re.sub(r'\\s+', ' ', text) # Removes extra whitespace from the text and replaces them with a space\n",
    "                                     # r'' means raw string\n",
    "                                     # \\s means any whitespace characters\n",
    "                                     # + means one or more occurences of the expression, i.e.: replace more than one instance of whitespace\n",
    "\n",
    "\n",
    "    return text.strip() # Removes any whitespace from the beginning and ends of text\n",
    "\n",
    "data['clean_review'] = data['review'].apply(clean_text) # Creates a new column in the dataframe called 'clean_review', which applies the clean_text function to data['review']\n",
    "print(data['clean_review'].head()) # Prints first 5 lines of data[clean_review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ae0496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for LSTM\n",
    "\n",
    "X = data['clean_review'] # Input data, i.e. the reviews that have been preprocessed, so data[clean_review]\n",
    "y = data['sentiment'].map({'positive': 1, 'negative': 0}) # Target labels, i.e.: the outcomes so data[sentiment]\n",
    "                                                          # Returns a copy of the sentiment labels in the dataframe by mapping positive and negative to 0 and 1.\n",
    "# Vectoriser\n",
    "max_words = 10000 # Maximum length of vocabulary, to save computational resources it only selects 10000 most useful words\n",
    "max_len = 200 # Maximum length of input, so dimension of embedding vectors is the same as input layer\n",
    "# If the input that is greater than max_len, the text is truncated (cut down to max_len)\n",
    "# If the input is less than max_len, the text is padded (use filler characters to increase length to max_len)\n",
    "\n",
    "# Implement a vectoriser using the TextVectorization Library\n",
    "# max_tokens represents the maximum size of the vocabulary, i.e.: max_words so 10000 words\n",
    "# sequence length sets the length of the input sequence to a specific length, i.e.: max_len so 200 words\n",
    "# Generates tokens and converts them to word vectors (not embeddings yet)\n",
    "vectoriser = TextVectorization(max_tokens=max_words, output_sequence_length=max_len)\n",
    "\n",
    "# Train-Test Split\n",
    "# Train set is the set used to fit the model.\n",
    "# Test set is the set used to evaluate a final model accurately (not part of the training set).\n",
    "# 20% of data is the test set so test_size is 0.2.\n",
    "# random_state is set to a number so that the splits generated are reproducible; otherwise the split would be generated randomly (a seed value)\n",
    "# Raw data means the data that has been preprocessed but not converted to embeddings\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "vectoriser.adapt(X_train_raw) # Creates a vocabulary (collection of frequent terms) of string terms from tokens in the X_train_raw data\n",
    "\n",
    "# Vectorize AFTER splitting -> this prevents data leakage so the model cannot learn patterns from the test data vocabulary and artificially boost the performance of the model. \n",
    "X_train = vectoriser(X_train_raw) # Applies vectoriser to X_train_raw\n",
    "X_test = vectoriser(X_test_raw) # Applies vectoriser to X_test_raw\n",
    "# The results are word vectors (words replaced with indices of where they appear in vocabulary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8edcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model\n",
    "\n",
    "model = Sequential() # Model should be sequential so that it is created layer by layer\n",
    "\n",
    "# Embedding layer added\n",
    "# This layer converts each word/token index (from the vocabulary) into a low-dimensional embedding vectors\n",
    "# The input dimension is the maximum possible word index, in other words, the size of the vocabulary: max_words = 10000\n",
    "# The output dimension is the dimension of the embedding vector for a word/token, 128\n",
    "model.add(Embedding(input_dim=max_words, output_dim=128))\n",
    "\n",
    "# LSTM layer added\n",
    "# 256 is the number of LSTM neurons\n",
    "# dropout = 20% of the neuron connections are deactivated in this layer to prevent overfitting\n",
    "# recurrent_dropout = 20% of the recurrent connections (weighted activations from last word/timestep) are deactivated in this layer to prevent overfitting\n",
    "model.add(LSTM(256, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "# Output layer is a dense layer (fully connected to previous layer).\n",
    "# In this case, this layer has 1 neuron and uses a sigmoid activation function.\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile Model for training\n",
    "# Model uses binary cross entropy loss, an ADAM optimiser and tracks accuracy\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6287817c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 329ms/step - accuracy: 0.5218 - loss: 0.6927 - val_accuracy: 0.5433 - val_loss: 0.6802\n",
      "Epoch 2/5\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 320ms/step - accuracy: 0.5755 - loss: 0.6720 - val_accuracy: 0.5608 - val_loss: 0.6758\n",
      "Epoch 3/5\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 323ms/step - accuracy: 0.7244 - loss: 0.5420 - val_accuracy: 0.8545 - val_loss: 0.3456\n",
      "Epoch 4/5\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 332ms/step - accuracy: 0.8798 - loss: 0.2941 - val_accuracy: 0.8725 - val_loss: 0.2995\n",
      "Epoch 5/5\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 346ms/step - accuracy: 0.9170 - loss: 0.2177 - val_accuracy: 0.8752 - val_loss: 0.3450\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x22c5e21e060>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Model\n",
    "\n",
    "# Trains the model for 5 epochs\n",
    "# batch_size represents the number of samples (rows of data) per gradient update (i.e.: updating model parameters)\n",
    "# validation_split represents the fraction of training data that will be used as validation_data (which is not trained but used to evaluate the loss of a model as well as any metrics like accuracy)\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0539d4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 24ms/step\n",
      "Accuracy: 0.8698\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.91      0.87      4961\n",
      "           1       0.90      0.83      0.87      5039\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Model\n",
    "\n",
    "y_pred_prob = model.predict(X_test) # Creates a probability matrix based on model predictions from X_test\n",
    "y_pred = (y_pred_prob > 0.5).astype(int) # Creates a boolean matrix based on the labels from y_pred_prob.\n",
    "                                         # Probabilities greater than 0.5 in the matrix correspond to 1 and the rest correspond to 0\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred)) # Generates accuracy by comparing y_test (target labels) from y_pred (model prediction of the target labels)\n",
    "print(classification_report(y_test, y_pred)) # Generates classification report that uses different performance metrics such as precision, recall, f1-score and support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51912b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "\n",
    "# Import Libraries: pandas, numpy, re, tensorflow and sk-learn\n",
    "\n",
    "# 1.  Read data from csv file\n",
    "\n",
    "# 2.  Preprocess data and create a new data frame column to store the preprocessed data\n",
    "        # Main steps include converting to lowercase and removing HTML tags, non-letter characters and whitespace.\n",
    "\n",
    "# 3.  Set your input values and target labels (remember to map target labels into numbers).\n",
    "\n",
    "# 4.  Create variables for your vectoriser: max_words, max_len.\n",
    "\n",
    "# 5.  Create vectoriser with the parameters: max_tokens, output_sequence_length.\n",
    "\n",
    "# 6.  Create train-test split: remember test_size and random_state and that X for this is raw data.\n",
    "\n",
    "# 7.  Adapt vectoriser so it creates a vocabulary.\n",
    "\n",
    "# 8.  Set X_train and X_test by using the vectoriser.\n",
    "\n",
    "# 9.  Create a model by doing the following:\n",
    "        # 1. Make model sequential.\n",
    "        # 2. Add embedding layer (with parameters input_dim and output_dim).\n",
    "        # 3. Create LSTM layer (with parameters: number of neurons, dropout and recurrent dropout).\n",
    "        # 4. Add output/dense layer (with parameters number of neurons and activation).\n",
    "        # 5. Compile model by using loss, optimiser and metrics as parameters.\n",
    "\n",
    "# 10. Train model with parameters: epochs, batch size and validation split\n",
    "\n",
    "# 11. Create two variables: y_pred_prob and y_pred.\n",
    "\n",
    "# 12. Use y_pred_prob and y_pred to compare with y_test in terms of accuracy score and classification report.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
