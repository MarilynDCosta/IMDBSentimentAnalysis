{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f0fc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable tensorflow warnings:\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0216f909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras.utils import pad_sequences\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4366b41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5105e189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/IMDB_Dataset.csv\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d39d6237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0    one of the other reviewers has mentioned that ...\n",
      "1    a wonderful little production the filming tech...\n",
      "2    i thought this was a wonderful way to spend ti...\n",
      "3    basically there s a family where a little boy ...\n",
      "4    petter mattei s love in the time of money is a...\n",
      "Name: clean_review, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('<.*?>', '', text) # removes HTML tags\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text) # removes special characters\n",
    "    text = re.sub(r'\\s+', ' ', text) # removes extra whitespace\n",
    "    return text.strip()\n",
    "\n",
    "data['clean_review'] = data['review'].apply(clean_text)\n",
    "print('\\n', data['clean_review'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7ae0496",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Prepare data for LSTM\n",
    "X = data['clean_review'] # input features\n",
    "y = data['sentiment'].map({'positive': 1, 'negative': 0}) # target labels\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#   Tokenisation\n",
    "max_words = 10000\n",
    "max_len = 200\n",
    "\n",
    "tokeniser = TextVectorization(max_tokens=max_words, output_sequence_length=max_len)\n",
    "\n",
    "# Train-Test Split (on raw text first)\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "tokeniser.adapt(X_train_raw)\n",
    "\n",
    "# Vectorize AFTER splitting\n",
    "X_train = tokeniser(X_train_raw)\n",
    "X_test = tokeniser(X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd8edcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=128)) # Turns words into dense vectors\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2)) # Learns sequential patterns in text\n",
    "model.add(Dense(1, activation='sigmoid')) # Outputs final sentiment prediction: 0 or 1\n",
    "\n",
    "# Compile Model\n",
    "# Model uses binary cross entropy loss, an ADAM optimiser and tracks accuracy by using metrics\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6287817c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 132ms/step - accuracy: 0.5766 - loss: 0.6771 - val_accuracy: 0.5395 - val_loss: 0.6860\n",
      "Epoch 2/5\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 131ms/step - accuracy: 0.6416 - loss: 0.6316 - val_accuracy: 0.6745 - val_loss: 0.6171\n",
      "Epoch 3/5\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 131ms/step - accuracy: 0.7485 - loss: 0.5141 - val_accuracy: 0.8410 - val_loss: 0.3933\n",
      "Epoch 4/5\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 131ms/step - accuracy: 0.8690 - loss: 0.3336 - val_accuracy: 0.8432 - val_loss: 0.4160\n",
      "Epoch 5/5\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 131ms/step - accuracy: 0.9018 - loss: 0.2685 - val_accuracy: 0.8717 - val_loss: 0.3303\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x248e57b2960>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Model\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0539d4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 23ms/step\n",
      "Accuracy: 0.8675\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.86      0.87      4961\n",
      "           1       0.86      0.88      0.87      5039\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Model\n",
    "\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7311cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\n",
      "Predicted Sentiment: Positive\n",
      "Confidence: 0.63\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\n",
      "Predicted Sentiment: Negative\n",
      "Confidence: 0.06\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\n",
      "Predicted Sentiment: Positive\n",
      "Confidence: 0.65\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\n",
      "Predicted Sentiment: Positive\n",
      "Confidence: 0.65\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\n",
      "Predicted Sentiment: Positive\n",
      "Confidence: 0.93\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\n",
      "Predicted Sentiment: Positive\n",
      "Confidence: 0.91\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\n",
      "Predicted Sentiment: Negative\n",
      "Confidence: 0.20\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\n",
      "Predicted Sentiment: Negative\n",
      "Confidence: 0.11\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\n",
      "Predicted Sentiment: Positive\n",
      "Confidence: 0.85\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\n",
      "Predicted Sentiment: Positive\n",
      "Confidence: 0.65\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\n",
      "Predicted Sentiment: Positive\n",
      "Confidence: 0.65\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Making Predictions\n",
    "\n",
    "# while True:\n",
    "#     user_input = input(\"Enter a movie review (or type 'exit' to quit): \\n\")\n",
    "#     if user_input.lower() == 'exit':\n",
    "#         break\n",
    "\n",
    "#     # Preprocess text\n",
    "#     cleaned_input = clean_text(user_input)\n",
    "\n",
    "#     # Vectorise text (already tokenised and padded to max_len)\n",
    "#     input_seq = tokeniser([cleaned_input])\n",
    "\n",
    "#     # Predict\n",
    "#     prediction = model.predict(input_seq)[0][0]\n",
    "#     sentiment = \"Positive\" if prediction > 0.5 else \"Negative\"\n",
    "\n",
    "#     print(f\"\\nPredicted Sentiment: {sentiment}\")\n",
    "#     print(f\"Confidence: {prediction:.2f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
