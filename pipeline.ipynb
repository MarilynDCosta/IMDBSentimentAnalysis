{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f0fc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable tensorflow warnings:\n",
    "import os # imports OS library which can perform OS commands\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # sets environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0216f909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Needed to store data as a dataframe for data preprocessing\n",
    "import numpy as np\n",
    "import re # Used to generate regex expressions\n",
    "import tensorflow as tf # Import ML functionality\n",
    "from tensorflow.keras.layers import TextVectorization # Creates a vectoriser -> converts tokens into model embeddings\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential # Used for creating model based on its layers\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense # Functionality for creating specific layers used in model\n",
    "from sklearn.model_selection import train_test_split # Used for creating a train-test split\n",
    "from sklearn.metrics import classification_report, accuracy_score # Used for performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4366b41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "# Prints version of TensorFlow\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5105e189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/IMDB_Dataset.csv\") # Reads data in the IMDB dataset and converts it into a dataframe\n",
    "print(data.head()) # Prints the first 5 lines of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39d6237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    one of the other reviewers has mentioned that ...\n",
      "1    a wonderful little production the filming tech...\n",
      "2    i thought this was a wonderful way to spend ti...\n",
      "3    basically there s a family where a little boy ...\n",
      "4    petter mattei s love in the time of money is a...\n",
      "Name: clean_review, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    # Cleans the text\n",
    "    text = text.lower() # Converts text to lower case\n",
    "\n",
    "    # re is a regex library\n",
    "    text = re.sub('<.*?>', '', text) # Removes HTML tags from the text (replaces them with empty string)\n",
    "                                     # <> is a typical structure for HTML tags\n",
    "                                     # . means that any character but new line would be removed\n",
    "                                     # * means 0 or more characters so .* means to remove all characters within <> (i.e.: the full HTML tag)\n",
    "                                     # ? specifies that you want to remove the HTML tags in a non-greedy way -> removes smallest possible pattern (i.e.: does not remove text inbetween HTML tags)\n",
    "\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text) # Removes special characters from the text and replaces them with a space\n",
    "                                          # [^a-zA-Z] Removes any characters that are not (^) lower (a-z) or uppercase (A-Z) letters\n",
    "\n",
    "\n",
    "    text = re.sub(r'\\s+', ' ', text) # Removes extra whitespace from the text and replaces them with a space\n",
    "                                     # r'' means raw string\n",
    "                                     # \\s means any whitespace characters\n",
    "                                     # + means one or more occurences of the expression, i.e.: replace more than one instance of whitespace\n",
    "\n",
    "\n",
    "    return text.strip() # Removes any whitespace from the beginning and ends of text\n",
    "\n",
    "data['clean_review'] = data['review'].apply(clean_text) # Creates a new column in the dataframe called 'clean_review', which applies the clean_text function to data['review']\n",
    "print(data['clean_review'].head()) # Prints first 5 lines of data[clean_review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ae0496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for LSTM\n",
    "\n",
    "X = data['clean_review'] # Input features, i.e. the reviews that have been preprocessed, so data[clean_review]\n",
    "y = data['sentiment'].map({'positive': 1, 'negative': 0}) # Target labels, i.e.: the outcomes so data[sentiment]\n",
    "                                                          # Maps the sentiment labels in the dataframe (positive and negative) to 0 and 1.\n",
    "\n",
    "# Vectoriser\n",
    "max_words = 10000 # Maximum length of vocabulary\n",
    "max_len = 200 # Maximum length of output\n",
    "# If the input that is greater than max_len, the text is truncated (cut down to max_len)\n",
    "# If the input is less than max_len, the text is padded (use filler characters to increase length to max_len)\n",
    "\n",
    "# Implement a vectoriser using the TextVectorization Library\n",
    "# max_tokens represents the maximum size of the vocabulary, i.e.: max_words so 10000 words\n",
    "# output_sequence_length sets the length of the output sequence to a specific length, i.e.: max_len so 200 words\n",
    "vectoriser = TextVectorization(max_tokens=max_words, output_sequence_length=max_len)\n",
    "\n",
    "# Train-Test Split\n",
    "# Train set is the set used to fit the model.\n",
    "# Test set is the set used to evaluate a final model accurately (not part of the training set).\n",
    "# 20% of data is the test set so test_size is 0.2.\n",
    "# random_state is set to a number so that the splits generated are reproducible; otherwise the split would be generated randomly (a seed value)\n",
    "# Raw data means the data that has been preprocessed but has not be \n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "vectoriser.adapt(X_train_raw) # Creates a vocabulary (collection of frequent terms) of string terms from tokens in the X_train_raw data\n",
    "\n",
    "# Vectorize AFTER splitting -> \n",
    "X_train = vectoriser(X_train_raw) # Applies vectoriser to X_train_raw\n",
    "X_test = vectoriser(X_test_raw) # Applies vectoriser to X_test_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dd8edcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model\n",
    "\n",
    "model = Sequential() # Model should be sequential so that it is created layer by layer\n",
    "\n",
    "# Embedding layer added\n",
    "# This layer converts the high-dimensional tokenised text into a low-dimensional vector\n",
    "# The input dimension in this case is the maximum tokens in our input, so 10000.\n",
    "# The output dimension is the dimension the vector should be\n",
    "model.add(Embedding(input_dim=max_words, output_dim=128))\n",
    "\n",
    "# LSTM layer added\n",
    "# 128 units -> this represents the dimensionality of the output space.\n",
    "# dropout = 0.2 means that 20% of units would be dropped for the linear transformation of the inputs.\n",
    "# recurrent_dropout = 0.2 implies that 20% of units will be dropped for linear transformations in the recurrent states.\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "# Dense layer added to feed all outputs from the previous layer to all neurons\n",
    "# In this case, this layer has 1 neuron and uses a sigmoid activation function.\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile Model for training\n",
    "# Model uses binary cross entropy loss, an ADAM optimiser and tracks accuracy\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6287817c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 128ms/step - accuracy: 0.5694 - loss: 0.6760 - val_accuracy: 0.6133 - val_loss: 0.6700\n",
      "Epoch 2/5\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 142ms/step - accuracy: 0.6332 - loss: 0.6436 - val_accuracy: 0.6463 - val_loss: 0.6390\n",
      "Epoch 3/5\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 124ms/step - accuracy: 0.7308 - loss: 0.5461 - val_accuracy: 0.8468 - val_loss: 0.3734\n",
      "Epoch 4/5\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 124ms/step - accuracy: 0.8712 - loss: 0.3172 - val_accuracy: 0.8730 - val_loss: 0.3071\n",
      "Epoch 5/5\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 123ms/step - accuracy: 0.9081 - loss: 0.2366 - val_accuracy: 0.8773 - val_loss: 0.3167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x22bb84ac4a0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Model\n",
    "\n",
    "# Trains the model for 5 epochs\n",
    "# batch_size represents the number of samples (rows of data) per gradient update (i.e.: updating model parameters)\n",
    "# validation_split represents the fraction of training data that will be used as validation_data (which is not trained but used to evaluate the loss of a model as well as any metrics like accuracy)\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0539d4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 26ms/step\n",
      "Accuracy: 0.8749\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.88      0.87      4961\n",
      "           1       0.88      0.87      0.87      5039\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Model\n",
    "\n",
    "y_pred_prob = model.predict(X_test) # Creates a probability based on model predictions from X_test\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred)) # Generates accuracy by comparing y_test (target labels) from y_pred (model prediction of the target labels)\n",
    "print(classification_report(y_test, y_pred)) # Generates classification report that uses different performance metrics such as precision, recall, f1-score and support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fc7311cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Predictions\n",
    "\n",
    "# while True:\n",
    "#     user_input = input(\"Enter a movie review (or type 'exit' to quit): \\n\")\n",
    "#     if user_input.lower() == 'exit':\n",
    "#         break\n",
    "\n",
    "#     # Preprocess text\n",
    "#     cleaned_input = clean_text(user_input)\n",
    "\n",
    "#     # Vectorise text (already tokenised and padded to max_len)\n",
    "#     input_seq = vectoriser([cleaned_input])\n",
    "\n",
    "#     # Predict\n",
    "#     prediction = model.predict(input_seq)[0][0]\n",
    "#     sentiment = \"Positive\" if prediction > 0.5 else \"Negative\"\n",
    "\n",
    "#     print(f\"\\nPredicted Sentiment: {sentiment}\")\n",
    "#     print(f\"Confidence: {prediction:.2f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
